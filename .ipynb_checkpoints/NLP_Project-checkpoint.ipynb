{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae550b0a-9322-410c-9d65-047ce6aa2945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0360eae4-90ac-4391-9d4f-eb89d3fd27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\remas\n",
      "[nltk_data]     alhazmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\remas\n",
      "[nltk_data]     alhazmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\remas\n",
      "[nltk_data]     alhazmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "Index(['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither',\n",
      "       'class', 'tweet'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\remas alhazmi\\\\OneDrive - University of Prince Mugrin\\\\Documents\\\\processed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     28\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mremas alhazmi\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive - University of Prince Mugrin\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDocuments\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mprocessed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Preprocessing complete. Saved to data/processed/preprocessed_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mremas alhazmi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive - University of Prince Mugrin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\anaconda\\envs\\env_tow\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\anaconda\\envs\\env_tow\\lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\anaconda\\envs\\env_tow\\lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\OneDrive\\anaconda\\envs\\env_tow\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\OneDrive\\anaconda\\envs\\env_tow\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\remas alhazmi\\\\OneDrive - University of Prince Mugrin\\\\Documents\\\\processed'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # For lemmatizer\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)         # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)            # Remove mentions\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)     # Remove everything except letters and spaces\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    tokens = [WordNetLemmatizer().lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(r\"C:\\Users\\remas alhazmi\\OneDrive - University of Prince Mugrin\\Desktop\\NLP\\labeled_data.csv\")\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    df['cleaned_text'] = df['tweet'].apply(clean_text)\n",
    "    df.to_csv(r\"C:\\Users\\remas alhazmi\\OneDrive - University of Prince Mugrin\\Documents\\processed\", index=False)\n",
    "    print(\"[INFO] Preprocessing complete. Saved to data/processed/preprocessed_data.csv\")\n",
    "\n",
    "\n",
    "    df_cleaned = pd.read_csv(r\"C:\\Users\\remas alhazmi\\OneDrive - University of Prince Mugrin\\Documents\\processed\")\n",
    "    for idx in range(5):\n",
    "        print(\"Original:\", df_cleaned.loc[idx, 'tweet'])\n",
    "        print(\"Cleaned:\", df_cleaned.loc[idx, 'cleaned_text'])\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e937bc0c-bed8-43a8-853c-d3b2883c5aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Generating BoW features...\n",
      "[INFO] Bag of Words features saved!\n",
      "[INFO] Generating TF-IDF features...\n",
      "[INFO] TF-IDF features saved!\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“„ src/feature_engineering.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "CLEANED_DATA_PATH = (r'C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\preprocessed_data.csv')\n",
    "FEATURES_SAVE_PATH = (r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\")\n",
    "\n",
    "def load_cleaned_data():\n",
    "    df = pd.read_csv(CLEANED_DATA_PATH)\n",
    "    # Drop any rows with missing or empty cleaned_text\n",
    "    df = df.dropna(subset=['cleaned_text'])\n",
    "    df = df[df['cleaned_text'].str.strip() != '']  # Also remove rows where cleaned_text is empty after stripping spaces\n",
    "    return df\n",
    "\n",
    "def generate_bow_features(texts):\n",
    "    vectorizer = CountVectorizer(max_features=5000)  # You can adjust max_features\n",
    "    X_bow = vectorizer.fit_transform(texts)\n",
    "    return X_bow, vectorizer\n",
    "\n",
    "def generate_tfidf_features(texts):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_tfidf = vectorizer.fit_transform(texts)\n",
    "    return X_tfidf, vectorizer\n",
    "\n",
    "def save_features(X, vectorizer, prefix):\n",
    "    os.makedirs(FEATURES_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "    # Save feature matrix\n",
    "    with open(os.path.join(FEATURES_SAVE_PATH, f\"{prefix}_features.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(X, f)\n",
    "\n",
    "    # Save vectorizer\n",
    "    with open(os.path.join(FEATURES_SAVE_PATH, f\"{prefix}_vectorizer.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_cleaned_data()\n",
    "\n",
    "    print(\"[INFO] Generating BoW features...\")\n",
    "    X_bow, bow_vectorizer = generate_bow_features(df['cleaned_text'])\n",
    "    save_features(X_bow, bow_vectorizer, \"bow\")\n",
    "    print(\"[INFO] Bag of Words features saved!\")\n",
    "\n",
    "    print(\"[INFO] Generating TF-IDF features...\")\n",
    "    X_tfidf, tfidf_vectorizer = generate_tfidf_features(df['cleaned_text'])\n",
    "    save_features(X_tfidf, tfidf_vectorizer, \"tfidf\")\n",
    "    print(\"[INFO] TF-IDF features saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40fa0efe-6111-487f-80e6-c24a1c1ec7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] POS n-gram features complete. Saved at: C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\preprocessed_with_pos_features.csv\n",
      "                                        cleaned_text  \\\n",
      "0  rt woman shouldnt complain cleaning house amp ...   \n",
      "1  rt boy dat coldtyga dwn bad cuffin dat hoe st ...   \n",
      "2  rt dawg rt ever fuck bitch start cry confused ...   \n",
      "3                                rt look like tranny   \n",
      "4  rt shit hear might true might faker bitch told ya   \n",
      "\n",
      "                                        POS_unigrams  \\\n",
      "0  [., ., ., NNP, NNP, RB, :, IN, DT, NN, PRP, MD...   \n",
      "1  [., ., ., ., ., NNP, JJ, NN, :, NN, NNS, VBP, ...   \n",
      "2  [., ., ., ., ., ., ., NNP, NNP, NNP, NNP, ., ....   \n",
      "3  [., ., ., ., ., ., ., ., ., NNP, NNP, NNP, :, ...   \n",
      "4  [., ., ., ., ., ., ., ., ., ., ., ., ., NNP, J...   \n",
      "\n",
      "                                         POS_bigrams  \\\n",
      "0  [(., .), (., .), (., NNP), (NNP, NNP), (NNP, R...   \n",
      "1  [(., .), (., .), (., .), (., .), (., NNP), (NN...   \n",
      "2  [(., .), (., .), (., .), (., .), (., .), (., ....   \n",
      "3  [(., .), (., .), (., .), (., .), (., .), (., ....   \n",
      "4  [(., .), (., .), (., .), (., .), (., .), (., ....   \n",
      "\n",
      "                                        POS_trigrams  \\\n",
      "0  [(., ., .), (., ., NNP), (., NNP, NNP), (NNP, ...   \n",
      "1  [(., ., .), (., ., .), (., ., .), (., ., NNP),...   \n",
      "2  [(., ., .), (., ., .), (., ., .), (., ., .), (...   \n",
      "3  [(., ., .), (., ., .), (., ., .), (., ., .), (...   \n",
      "4  [(., ., .), (., ., .), (., ., .), (., ., .), (...   \n",
      "\n",
      "                                 POS_sequence_string  \n",
      "0  . . . NNP NNP RB : IN DT NN PRP MD RB VB IN VB...  \n",
      "1  . . . . . NNP JJ NN : NN NNS VBP : NN NN JJ IN...  \n",
      "2  . . . . . . . NNP NNP NNP NNP . . . . NNP VBD ...  \n",
      "3  . . . . . . . . . NNP NNP NNP : NN VBD PRP VBP...  \n",
      "4  . . . . . . . . . . . . . NNP JJ NNS : DT NN P...  \n"
     ]
    }
   ],
   "source": [
    "# ðŸ“„ src/pos_feature_engineering.py\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "POS_TAGGED_PATH = r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\preprocessed_with_pos.csv\"\n",
    "FEATURE_SAVE_PATH = r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\preprocessed_with_pos_features.csv\"\n",
    "\n",
    "# Load the POS-tagged dataset\n",
    "df = pd.read_csv(POS_TAGGED_PATH)\n",
    "\n",
    "# Function to safely convert strings back to lists if needed\n",
    "import ast\n",
    "df['POS_tags_nltk'] = df['POS_tags_nltk'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Create POS unigrams\n",
    "df['POS_unigrams'] = df['POS_tags_nltk']\n",
    "\n",
    "# Create POS bigrams\n",
    "df['POS_bigrams'] = df['POS_tags_nltk'].apply(lambda tags: list(ngrams(tags, 2)))\n",
    "\n",
    "# Create POS trigrams\n",
    "df['POS_trigrams'] = df['POS_tags_nltk'].apply(lambda tags: list(ngrams(tags, 3)))\n",
    "\n",
    "# Create full POS sequence string\n",
    "df['POS_sequence_string'] = df['POS_tags_nltk'].apply(lambda tags: ' '.join(tags))\n",
    "\n",
    "# Save the dataframe with POS n-gram features\n",
    "os.makedirs(os.path.dirname(FEATURE_SAVE_PATH), exist_ok=True)\n",
    "df.to_csv(FEATURE_SAVE_PATH, index=False)\n",
    "print(f\"[INFO] POS n-gram features complete. Saved at: {FEATURE_SAVE_PATH}\")\n",
    "\n",
    "# Show sample\n",
    "print(df[['cleaned_text', 'POS_unigrams', 'POS_bigrams', 'POS_trigrams', 'POS_sequence_string']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08cee440-6863-49ba-9ee2-ca0486eb7a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] POS-TFIDF feature shape: (24783, 34)\n",
      "[INFO] POS TF-IDF features and vectorizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "POS_FEATURES_SAVE_PATH = r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\"\n",
    "df = pd.read_csv(r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\preprocessed_with_pos_features.csv\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_pos_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform on POS sequences\n",
    "X_pos_tfidf = tfidf_pos_vectorizer.fit_transform(df['POS_sequence_string'].fillna(''))\n",
    "\n",
    "print(f\"[INFO] POS-TFIDF feature shape: {X_pos_tfidf.shape}\")\n",
    "\n",
    "# Save feature matrix\n",
    "with open(os.path.join(POS_FEATURES_SAVE_PATH, \"pos_tfidf_features.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(X_pos_tfidf, f)\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "with open(os.path.join(POS_FEATURES_SAVE_PATH, \"pos_tfidf_vectorizer.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(tfidf_pos_vectorizer, f)\n",
    "\n",
    "print(\"[INFO] POS TF-IDF features and vectorizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82352e23-779a-4aec-9631-222f0ad5344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 400000 word vectors from GloVe.\n",
      "[INFO] GloVe embeddings shape: (24783, 300)\n",
      "[INFO] Tweet GloVe Embeddings saved at: C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\tweet_glove_embeddings_300d.pkl\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“„ src/glove_embeddings.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\preprocessed_with_pos_features.csv\"\n",
    "GLOVE_PATH = r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\glove.6B\\glove.6B.300d.txt\"  # Path to GloVe file\n",
    "OUTPUT_EMBEDDINGS_PATH = r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\tweet_glove_embeddings_300d.pkl\"\n",
    "\n",
    "# Load preprocessed tweets\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Function to load GloVe embeddings\n",
    "def load_glove_embeddings(glove_path):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coeffs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coeffs\n",
    "    print(f\"[INFO] Loaded {len(embeddings_index)} word vectors from GloVe.\")\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe vectors\n",
    "embeddings_index = load_glove_embeddings(GLOVE_PATH)\n",
    "\n",
    "# Function to get average GloVe vector for a tweet\n",
    "def get_average_glove_vector(text, embeddings_index, embedding_dim=300):\n",
    "    words = str(text).split()  # Split tweet into words\n",
    "    valid_vectors = [embeddings_index[word] for word in words if word in embeddings_index]\n",
    "    if valid_vectors:\n",
    "        return np.mean(valid_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)\n",
    "\n",
    "# Apply function to every tweet\n",
    "tweet_embeddings = df['cleaned_text'].apply(lambda x: get_average_glove_vector(x, embeddings_index))\n",
    "\n",
    "# Convert to numpy array\n",
    "X_glove_embeddings = np.stack(tweet_embeddings.values)\n",
    "\n",
    "print(f\"[INFO] GloVe embeddings shape: {X_glove_embeddings.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "with open(OUTPUT_EMBEDDINGS_PATH, \"wb\") as f:\n",
    "    pickle.dump(X_glove_embeddings, f)\n",
    "\n",
    "print(f\"[INFO] Tweet GloVe Embeddings saved at: {OUTPUT_EMBEDDINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b0b808-b256-4686-8f3c-46b223781594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training GRU model...\n",
      "Epoch 1/10\n",
      "279/279 [==============================] - 10s 15ms/step - loss: 0.4199 - accuracy: 0.8429 - val_loss: 0.3208 - val_accuracy: 0.8734\n",
      "Epoch 2/10\n",
      "279/279 [==============================] - 2s 8ms/step - loss: 0.3404 - accuracy: 0.8722 - val_loss: 0.3085 - val_accuracy: 0.8739\n",
      "Epoch 3/10\n",
      "279/279 [==============================] - 2s 9ms/step - loss: 0.3251 - accuracy: 0.8742 - val_loss: 0.3062 - val_accuracy: 0.8754\n",
      "Epoch 4/10\n",
      "279/279 [==============================] - 2s 8ms/step - loss: 0.3168 - accuracy: 0.8811 - val_loss: 0.3060 - val_accuracy: 0.8770\n",
      "Epoch 5/10\n",
      "279/279 [==============================] - 3s 9ms/step - loss: 0.3088 - accuracy: 0.8796 - val_loss: 0.3026 - val_accuracy: 0.8805\n",
      "Epoch 6/10\n",
      "279/279 [==============================] - 2s 8ms/step - loss: 0.3035 - accuracy: 0.8831 - val_loss: 0.3019 - val_accuracy: 0.8815\n",
      "Epoch 7/10\n",
      "279/279 [==============================] - 2s 8ms/step - loss: 0.2948 - accuracy: 0.8855 - val_loss: 0.3004 - val_accuracy: 0.8810\n",
      "Epoch 8/10\n",
      "279/279 [==============================] - 2s 8ms/step - loss: 0.2927 - accuracy: 0.8868 - val_loss: 0.2994 - val_accuracy: 0.8815\n",
      "Epoch 9/10\n",
      "279/279 [==============================] - 2s 8ms/step - loss: 0.2888 - accuracy: 0.8861 - val_loss: 0.2994 - val_accuracy: 0.8845\n",
      "Epoch 10/10\n",
      "279/279 [==============================] - 2s 8ms/step - loss: 0.2816 - accuracy: 0.8893 - val_loss: 0.2941 - val_accuracy: 0.8835\n",
      "[INFO] Evaluating on test set...\n",
      "155/155 [==============================] - 2s 3ms/step\n",
      "[[  83  172   35]\n",
      " [  78 3600  154]\n",
      " [  12  146  677]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.29      0.36       290\n",
      "           1       0.92      0.94      0.93      3832\n",
      "           2       0.78      0.81      0.80       835\n",
      "\n",
      "    accuracy                           0.88      4957\n",
      "   macro avg       0.73      0.68      0.69      4957\n",
      "weighted avg       0.87      0.88      0.87      4957\n",
      "\n",
      "[INFO] GRU model saved at: C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\models\\gru_model.h5\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“„ src/train_gru_model.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "LABELS_PATH = r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\preprocessed_with_pos_features.csv\"\n",
    "EMBEDDINGS_PATH = r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\tweet_glove_embeddings_300d.pkl\"\n",
    "MODEL_SAVE_PATH = r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\models\\gru_model.h5\"\n",
    "\n",
    "# Load labels\n",
    "df = pd.read_csv(LABELS_PATH)\n",
    "y = df['class'].values\n",
    "y_cat = to_categorical(y, num_classes=3)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "with open(EMBEDDINGS_PATH, \"rb\") as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build GRU model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X.shape[1],)))  # Input is already averaged (300d)\n",
    "model.add(tf.keras.layers.Reshape((1, X.shape[1])))  # Reshape to feed into GRU\n",
    "model.add(GRU(128, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(\"[INFO] Training GRU model...\")\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "print(\"[INFO] Evaluating on test set...\")\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Save model\n",
    "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"[INFO] GRU model saved at: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c049f-dccd-4a8e-9508-f4742d08f896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sana\\anaconda3\\envs\\tfenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Sana\\anaconda3\\envs\\tfenv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sana\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24783/24783 [00:08<00:00, 2797.50 examples/s]\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# Import Required Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "# Load Preprocessed Dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Sana\\Downloads\\Hate_Speech_Detection\\data\\processed\\preprocessed_with_pos_features.csv\")\n",
    "\n",
    "# Only keep the tweet and class label columns\n",
    "df = df[['tweet', 'class']]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Convert to Huggingface Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load BERT Tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the Dataset\n",
    "def tokenize(example):\n",
    "    return tokenizer(example['tweet'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Train-Test Split\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Load Pre-trained BERT Model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"bert_logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_test['train'],\n",
    "    eval_dataset=train_test['test'],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the Model\n",
    "predictions = trainer.predict(train_test['test'])\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = train_test['test']['class']\n",
    "\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Save the Fine-Tuned Model\n",
    "trainer.save_model(\"bert_model\")\n",
    "tokenizer.save_pretrained(\"bert_model\")\n",
    "print(\"[INFO] BERT model and tokenizer saved to ./bert_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742eeac-6b16-4ecf-bfe3-c4a98341d409",
   "metadata": {},
   "source": [
    "# Transformer Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a33e88-af28-4273-881d-69d7e7076aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f640849-baa2-4ae1-afa4-e85ffa245533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (make sure it's in a suitable format for BERT)\n",
    "dataset = load_dataset(\"your_dataset_name\")  # You may need to load a custom dataset\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_bert')\n",
    "tokenizer.save_pretrained('./fine_tuned_bert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2d687-b81f-4f02-9f59-75b99d37645b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b981debc-ebca-4fb0-aae4-ebaa3bbdc368",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150df89e-129c-43df-a6a6-ac8cdf81d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model for English\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_syntax_features(text):\n",
    "    doc = nlp(text)\n",
    "    # Extract POS tags, dependencies, and noun chunks\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    dependencies = [token.dep_ for token in doc]\n",
    "    noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n",
    "    \n",
    "    return {\n",
    "        \"pos_tags\": pos_tags,\n",
    "        \"dependencies\": dependencies,\n",
    "        \"noun_chunks\": noun_chunks\n",
    "    }\n",
    "\n",
    "# Apply syntax-aware features to the dataset\n",
    "df['syntax_features'] = df['cleaned_text'].apply(extract_syntax_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910784b-5511-48cd-a7ce-d4d6cfc7676b",
   "metadata": {},
   "source": [
    "# Evaluation Phase for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439690e-cf7e-46d8-bd6b-969acd8cab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets['test'])\n",
    "\n",
    "# Convert predictions to label indices\n",
    "preds = predictions.argmax(axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "\n",
    "# Display the results\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab8456-7a1f-41f9-9b96-a35aaf4242e1",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ca4e9-ba09-4a6e-9271-33750694e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix for BERT')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_tow]",
   "language": "python",
   "name": "conda-env-env_tow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
