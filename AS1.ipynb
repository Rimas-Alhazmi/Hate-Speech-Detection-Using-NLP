{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4fd1bdb-570d-43e6-8a54-2066bdc831ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\remas alhazmi\\onedrive\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.8 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.8/11.8 MB 20.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 25.4 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 632.6/632.6 kB 24.7 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 37.3 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 5.0/6.3 MB 27.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 20.2 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.4/5.4 MB 36.3 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd46559-64e2-410a-9b19-7dd2026b7397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     ------------- -------------------------- 4.2/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 27.7 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb14b20d-49d3-470e-ad18-9b8726c08a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy and the language model are working\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"SpaCy and the language model are working\")\n",
    "# Just to cheke it is workning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb6c06-79bf-47d7-beeb-c42c2a8a6e5e",
   "metadata": {},
   "source": [
    "# AS1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2558e4-4e18-4bb4-8ade-7bab28171aaa",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a726d190-a05a-4eba-92cb-4cdbdb39bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Read the contents of a text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be read.\n",
    "\n",
    "    Returns:\n",
    "        str: The contents of the file as a single string.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize the input text using spaCy's language model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The original text to lemmatize.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lemmatized words from the text.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "\n",
    "def save_to_file(words, file_path):\n",
    "    \"\"\"\n",
    "    Save a list of words to a file.\n",
    "\n",
    "    Args:\n",
    "        words (list): The list of words to save.\n",
    "        file_path (str): The path to the output file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(\" \".join(words))\n",
    "\n",
    "\n",
    "def count_changed_words(original, lemmatized):\n",
    "    \"\"\"\n",
    "    Count how many words changed after lemmatization.\n",
    "\n",
    "    Args:\n",
    "        original (list): The original list of words.\n",
    "        lemmatized (list): The lemmatized list of words.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of words that changed after lemmatization.\n",
    "    \"\"\"\n",
    "    return sum(1 for orig, lemma in zip(original, lemmatized) if orig != lemma)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to read, lemmatize, and save text,\n",
    "    and display the total number of changed words.\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    input_file = \"wiki_mountain_def.txt\"\n",
    "    output_file = \"lemmatized_output.txt\"\n",
    "\n",
    "    # Process\n",
    "    text = read_file(input_file)\n",
    "    original_words = text.split()\n",
    "    lemmatized_words = lemmatize_text(text)\n",
    "    save_to_file(lemmatized_words, output_file)\n",
    "\n",
    "    # Count and report changes\n",
    "    changed_words = count_changed_words(original_words, lemmatized_words)\n",
    "    print(f\"Lemmatization complete. Total words changed to : {changed_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31310546-620e-47f5-abe8-a026ef567aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization complete. Total words changed to : 225\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe8d16-9d8b-4ece-926e-e477685250ba",
   "metadata": {},
   "source": [
    "# Stemming Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f80eb0-538d-43d1-b723-2d2bbb26d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stem(word):\n",
    "    \"\"\"\n",
    "    Apply a subset of the Porter stemming rules.\n",
    "\n",
    "    Rules applied:\n",
    "    - SSES -> SS  (e.g., caresses → caress)\n",
    "    - IES -> I    (e.g., ponies → poni)\n",
    "    - SS -> SS    (e.g., caress → caress)\n",
    "    - S -> (remove) (e.g., cats → cat)\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to be stemmed.\n",
    "\n",
    "    Returns:\n",
    "        str: The stemmed word according to the specified rules.\n",
    "    \"\"\"\n",
    "    if word.endswith(\"sses\"):\n",
    "        return word[:-2]  # Remove 'es' from 'sses'\n",
    "    elif word.endswith(\"ies\"):\n",
    "        return word[:-3] + \"i\"  # Replace 'ies' with 'i'\n",
    "    elif word.endswith(\"ss\"):\n",
    "        return word  # Leave 'ss' unchanged\n",
    "    elif word.endswith(\"s\"):\n",
    "        return word[:-1]  # Remove the trailing 's'\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def perform_stemming(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Read text from a file, apply stemming rules, save the output, and\n",
    "    count how many words were changed.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the original text file.\n",
    "        output_file (str): Path to save the stemmed output.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of words changed during stemming.\n",
    "    \"\"\"\n",
    "    # Read the original text\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    original_words = text.split()\n",
    "    stemmed_words = [porter_stem(word) for word in original_words]\n",
    "\n",
    "    # Save the stemmed words to a file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" \".join(stemmed_words))\n",
    "\n",
    "    # Count the number of words that changed after stemming\n",
    "    changed_count = sum(1 for orig, stem in zip(original_words, stemmed_words) if orig != stem)\n",
    "    return changed_count\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to perform stemming and display the number of changed words.\n",
    "    \"\"\"\n",
    "    input_file = \"wiki_mountain_def.txt\"\n",
    "    output_file = \"stemmed_output.txt\"\n",
    "\n",
    "    changed_words = perform_stemming(input_file, output_file)\n",
    "    print(f\"Stemming complete. Total words changed to: {changed_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d7e7999-d28f-42bd-836e-2df7fa11a82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming complete. Total words changed to: 31\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d063c-18c9-4306-8e88-ef97585727db",
   "metadata": {},
   "source": [
    "# differences between lemmatization and stemming results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a160cf8-4f75-4ddd-a20e-9d19e87309aa",
   "metadata": {},
   "source": [
    "# Two important methods in natural language processing (NLP) for breaking down words into their basic forms are lemmatization and stemming.  Despite having a similar objective, their approaches to achieving it are very different.  By considering the context and part of speech, lemmatization converts words into their fundamental dictionary form, or lemma.  For instance, \"better\" would become \"good,\" while \"running\" would lemmatize to \"run.\"  By doing this, the words' actual meanings are preserved.  Lemmatization is more accurate but may take longer to process because it uses complex language models like SpaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d32ff6-d59c-4a85-a2d4-b24e3d6fdd3c",
   "metadata": {},
   "source": [
    "# In contrast, stemming removes suffixes from words using straightforward rule-based algorithms such as the Porter Stemmer.  This approach may result in errors since it ignores the context of the word.  For example, \"ponies\" may be mispronounced as \"poni,\" even while \"running\" is appropriately reduced to \"run.\"  Although stemming is quicker and more effective, accuracy is frequently sacrificed, and it occasionally creates word forms that don't exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84dc28f-c66a-4768-bcec-1e56ede08595",
   "metadata": {},
   "source": [
    "# Lemmatization is the superior option in real-world applications where meaning and accuracy are crucial, like in text analysis or machine learning models.  Stemming, however, works better in situations where speed is essential, such as search engines, when a small amount of error is acceptable.  Lemmatization altered 225 words in the assignment results, demonstrating its more thorough and accurate processing.  Stemming, on the other hand, only altered 31 words, demonstrating its more straightforward, rule-based methodology.  The decision between lemmatization and stemming ultimately comes down to whether the task requires quick processing or great accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
